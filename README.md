# StudyBuddy â€“ Assignment 2 (LLM App)

StudyBuddy is a lightweight local LLM-powered tutor for math, CS, and linear algebra.  
It supports Retrieval-Augmented Generation (RAG), includes guardrails, telemetry,
and an offline evaluation suite, and runs fully locally using Ollama.

---

## ðŸš€ Features

### âœ” Core LLM Feature
Users type a question into the frontend and receive a structured explanation generated
by a local LLM (Gemma 2B). The backend applies:
- A controlled system prompt
- Subject detection (math / CS / linear algebra)
- Safety guardrails
- Optional RAG to improve grounding

### âœ” Enhancement: RAG (Retrieval-Augmented Generation)
Relevant chunks are retrieved from `seed_notes.txt` using TF-IDF + cosine similarity.
The top-scoring chunks are injected into the prompt before sending to the LLM.

### âœ” Safety Guardrails
- Prompt-injection filter (blocks â€œignore previousâ€¦â€ etc.)
- Input length limit (max 500 chars)
- CORS restrictions
- Fallback error messages
- JSON-safe response handling

### âœ” Telemetry
Each request logs:
- Timestamp  
- Latency  
- Subject  
- User question  
- Pathway (`"RAG"`)  

Stored in `telemetry.log`.

### âœ” Offline Evaluation
A `tests.json` file (â‰¥15 tests) and a runner script `run_tests.py`
evaluate correctness using simple keyword pattern matching.

---

## ðŸ“¦ Project Structure

---
studybuddy/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ rag.py
â”‚ â”œâ”€â”€ seed_notes.txt
â”‚ â”œâ”€â”€ telemetry.log
â”‚ â”œâ”€â”€ tests.json
â”‚ â”œâ”€â”€ run_tests.py
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â”œâ”€â”€ .env.example
â”‚ â””â”€â”€ (optional venv)
â”‚
â””â”€â”€ frontend/
â”œâ”€â”€ index.html
â”œâ”€â”€ script.js
â””â”€â”€ styles.css
---


---

# ðŸ›  Installation & Running the App

## 1ï¸âƒ£ Backend Setup (FastAPI + Ollama)

### Install dependencies
```
cd backend
pip install -r requirements.txt
```

### Start the backend
`uvicorn main:app --reload`


### Make sure Ollama is running in the background  
Olama model pull:
`ollama pull gemma:2b-instruct`


---

## 2ï¸âƒ£ Frontend (No Live Server Needed)

### ðŸš¨ IMPORTANT NOTE ABOUT LIVE SERVER (BUG WARNING)

Visual Studio Code's Live Server **automatically reloads the page** when:
- Backend responses are slow (e.g., 10â€“20 seconds from Ollama)
- Files change
- The browser detects a stalled request  
- Long-running AI responses occur

This causes the **entire chat to refresh** and messages disappear.

### âœ… FIX: Open index.html *directly* from the filesystem

Instead of Live Server, use:

1. Navigate to the `frontend` folder  
2. **Double-click `index.html`**, or open it in your browser via:

file:///path/to/studybuddy/frontend/index.html


This is the intended way to run the frontend because:
- Our backend CORS allows file:// origin  
- No auto-refresh  
- No disappearing messages  
- Works consistently with Ollama  

> **Professor:** Please open `index.html` from the folder, not Live Server,
> to avoid the known Live Server auto-reload bug with long-running LLM responses.

---

# ðŸ§ª Offline Evaluation (Required for Assignment)

The `tests.json` file contains â‰¥15 tests.  
Run:

```
cd backend
python3 run_tests.py
```